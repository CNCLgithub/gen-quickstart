{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Generative Models in Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "**Section 1.** [Julia, Gen, and this Jupyter notebook](#julia-gen-jupyter)\n",
    "\n",
    "**Section 2.** [Writing a probabilistic model as a generative function](#writing-model)\n",
    "\n",
    "**Section 3.** [Doing posterior inference](#doing-inference)\n",
    "\n",
    "**Section 4.** [Predicting new data](#predicting-data)\n",
    "\n",
    "**Section 5.** [Generative models in visual object perception - a teaser](#shape-inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Julia, Gen, and this Jupyter notebook  <a name=\"julia-gen-jupyter\"></a>\n",
    "\n",
    "Gen is a package for the Julia language. The package can be loaded with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gen programs typically consist of a combination of (i) probabilistic models written in modeling languages and (ii) inference programs written in regular Julia code. Gen provides a built-in modeling language that is itself based on Julia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will make use of Julia symbols. Note that a Julia symbol is different from a Julia string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(:foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(\"foo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Writing a probabilistic model as a generative function  <a name=\"writing-model\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probabilistic models are represented in Gen as *generative functions*. In order for Gen to interpret Julia code as a generative model, we need to prefix the function with the `@gen` keyword.\n",
    "\n",
    "One of the most important things in Gen is the ***trace* data structure**. The trace stores all the random choices made by the generative model which is extremely useful when we are doing inference.\n",
    "\n",
    "To include a certain random choice in the generative model we need to add `@trace` keyword with a unique address. In the particular case below, addresses are Julia symbols (e.g. `:slope`) and tuples of symbols and integers (`(:y, i)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function line_model(xs::Vector{Float64})\n",
    "    n = length(xs)\n",
    "    \n",
    "    # We begin by sampling a slope and intercept for the line from the prior.\n",
    "    slope = @trace(normal(0, 1), :slope)\n",
    "    intercept = @trace(normal(0, 2), :intercept)\n",
    "    \n",
    "    # Given the slope and intercept, we can sample y coordinates\n",
    "    # for each of the x coordinates in our input vector.\n",
    "    for (i, x) in enumerate(xs)\n",
    "        @trace(normal(slope * x + intercept, 0.1), (:y, i))\n",
    "    end\n",
    "    \n",
    "    # Return value is often not particularly important.\n",
    "    return n\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Line Model](images/line_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [-5., -4., -3., -2., -1., 0., 1., 2., 3., 4., 5.];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random choices are included in the *execution trace* of the generative function. We can run the generative function and obtain its trace using the [`\n",
    "simulate`](https://probcomp.github.io/Gen/dev/ref/gfi/#Gen.simulate) method from the Gen API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = Gen.simulate(line_model, (xs,));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method takes the function to be executed, and a tuple of arguments to the function, and returns a trace and a second value that we will not be using in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Among other things, the trace contains the value of the random choices, stored in map from address to value called a *choice map*. This map is available through the API method [`get_choices`]():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(Gen.get_choices(trace))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pull out individual values from this map using Julia's subscripting syntax `[...]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = Gen.get_choices(trace)\n",
    "println(choices[:slope])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also read the value of a random choice directly from the trace, without having to use `get_choices` first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(trace[:slope])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is helpful to visualize traces of generative functions. The function below renders the x-y data points and the line that is represented by the slope and intercept choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function render_trace(trace; show_data=true)\n",
    "    \n",
    "    # Pull out xs from the trace\n",
    "    xs = get_args(trace)[1]\n",
    "    \n",
    "    xmin = minimum(xs)\n",
    "    xmax = maximum(xs)\n",
    "    if show_data\n",
    "        ys = [trace[(:y, i)] for i=1:length(xs)]\n",
    "        \n",
    "        # Plot the data set\n",
    "        scatter(xs, ys, c=\"black\")\n",
    "    end\n",
    "    \n",
    "    # Pull out slope and intercept from the trace\n",
    "    slope = trace[:slope]\n",
    "    intercept = trace[:intercept]\n",
    "    \n",
    "    # Draw the line\n",
    "    plot([xmin, xmax], slope *  [xmin, xmax] .+ intercept, color=\"black\", alpha=0.5)\n",
    "    ax = gca()\n",
    "    ax[:set_xlim]((xmin, xmax))\n",
    "    ax[:set_ylim]((xmin, xmax))\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(3,3))\n",
    "render_trace(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because a generative function is stochastic, we need to visualize many runs in order to understand its behavior. The cell below renders a grid of traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function grid(renderer::Function, traces; ncols=6, nrows=3)\n",
    "    figure(figsize=(16, 8))\n",
    "    for (i, trace) in enumerate(traces)\n",
    "        subplot(nrows, ncols, i)\n",
    "        renderer(trace)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate several traces and render them in a grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "traces = [Gen.simulate(line_model, (xs,)) for _=1:12]\n",
    "grid(render_trace, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Doing Posterior inference  <a name=\"doing-inference\"></a>\n",
    "\n",
    "We now will provide a data set of y-coordinates and try to draw inferences about the process that generated the data. We begin with the following data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = [6.75003, 6.1568, 4.26414, 1.84894, 3.09686, 1.94026, 1.36411, -0.83959, -0.976, -1.93363, -2.91303];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(3,3))\n",
    "scatter(xs, ys, color=\"black\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume that the line model was responsible for generating the data, and infer values of the slope and intercept that explain the data.\n",
    "\n",
    "Our simple *inference program* takes the model we are assuming generated our data, the data set, and the amount of computation to perform, and returns a trace that is approximately sampled from the _posterior distribution_ on traces of the function, given the observed data.\n",
    "\n",
    "Functions like `importance_resampling` expect us to provide a _model_ and also an _choice map_ representing our data set and relating it to the model. A choice map maps random choice addresses from the model to values from our data set. Here, we want to tie model addresses like `(:y, 4)` to data set values like `ys[4]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function do_inference(model, xs, ys, amount_of_computation)\n",
    "    \n",
    "    # Create a choice map that maps model addresses (:y, i)\n",
    "    # to observed values ys[i]. We leave :slope and :intercept\n",
    "    # unconstrained, because we want them to be inferred.\n",
    "    observations = Gen.choicemap()\n",
    "    for (i, y) in enumerate(ys)\n",
    "        observations[(:y, i)] = y\n",
    "    end\n",
    "    \n",
    "    # Call importance_resampling to obtain a likely trace consistent\n",
    "    # with our observations.\n",
    "    (trace, _) = Gen.importance_resampling(model, (xs,), observations, amount_of_computation);\n",
    "    return trace\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the inference program to obtain a trace, and then visualize the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = do_inference(line_model, xs, ys, 100)\n",
    "figure(figsize=(3,3))\n",
    "render_trace(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `importance_resampling` found a reasonable slope and intercept to explain the data. We can also visualize many samples in a grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [do_inference(line_model, xs, ys, 100) for _=1:10];\n",
    "grid(render_trace, traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that there is some uncertainty: with our limited data, we can't be 100% sure exactly where the line is. We can get a better sense for the variability in the posterior distribution by visualizing all the traces in one plot, rather than in a grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function overlay(renderer, traces; same_data=true, args...)\n",
    "    renderer(traces[1], show_data=true, args...)\n",
    "    for i=2:length(traces)\n",
    "        renderer(traces[i], show_data=!same_data, args...)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = [do_inference(line_model, xs, ys, 100) for _=1:10];\n",
    "figure(figsize=(3,3))\n",
    "overlay(render_trace, traces);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "The results above were obtained for `amount_of_computation = 100`. Run the algorithm with this value set to `1`, `10`, and `1000`, etc.  Which value seems like a good tradeoff between accuracy and running time? Discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predicting new data  <a name=\"predicting-data\"></a>\n",
    "\n",
    "Using the API method [`generate`](https://probcomp.github.io/Gen/dev/ref/gfi/#Gen.generate), we can generate a trace of a generative function in which the values of certain random choices are constrained to given values.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraints = Gen.choicemap()\n",
    "constraints[:slope] = 0.\n",
    "constraints[:intercept] = 0.\n",
    "(trace, _) = Gen.generate(line_model, (xs,), constraints)\n",
    "figure(figsize=(3,3))\n",
    "render_trace(trace);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use constraints to predict new data given inferred latents (e.g. inferred slope and intercept).\n",
    "\n",
    "The function below (`predict_new_data`) takes a trace, and a vector of new x-coordinates, and returns a vector of predicted y-coordinates corresponding to the x-coordinates in `new_xs`. This function works with multiple models, so the set of parameter addresses is an argument (`param_addrs`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function predict_new_data(model, trace, new_xs::Vector{Float64}, param_addrs)\n",
    "    \n",
    "    # Copy parameter values from the inferred trace (`trace`)\n",
    "    # into a fresh set of constraints.\n",
    "    constraints = Gen.choicemap()\n",
    "    for addr in param_addrs\n",
    "        constraints[addr] = trace[addr]\n",
    "    end\n",
    "    \n",
    "    # Run the model with new x coordinates, and with parameters \n",
    "    # fixed to be the inferred values\n",
    "    (new_trace, _) = Gen.generate(model, (new_xs,), constraints)\n",
    "    \n",
    "    # Pull out the y-values and return them\n",
    "    ys = [new_trace[(:y, i)] for i=1:length(new_xs)]\n",
    "    return ys\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below defines a function that first performs inference on an observed data set `(xs, ys)`, and then runs `predict_new_data` to generate predicted y-coordinates. It repeats this process `num_traces` times, and returns a vector of the resulting y-coordinate vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function infer_and_predict(model, xs, ys, new_xs, param_addrs, num_traces, amount_of_computation)\n",
    "    pred_ys = []\n",
    "    for i=1:num_traces\n",
    "        trace = do_inference(model, xs, ys, amount_of_computation)\n",
    "        push!(pred_ys, predict_new_data(model, trace, new_xs, param_addrs))\n",
    "    end\n",
    "    pred_ys\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a cell that plots the observed data set `(xs, ys)` as red dots, and the predicted data as small black dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_predictions(xs, ys, new_xs, pred_ys)\n",
    "    scatter(xs, ys, color=\"red\")\n",
    "    for pred_ys_single in pred_ys\n",
    "        scatter(new_xs, pred_ys_single, color=\"black\", s=1, alpha=0.3)\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the original dataset for the line model. The x-coordinates span the interval -5 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(3,3))\n",
    "scatter(xs, ys, color=\"red\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the inferred values of the parameters to predict ys in the interval -5 to 10. Predicting new data from inferred parameters, and comparing this new data to the observed data is the core idea behind *posterior predictive checking*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_xs = collect(range(-5, stop=10, length=100));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate and plot the predicted data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_ys = infer_and_predict(line_model, xs, ys, new_xs, [:slope, :intercept], 20, 1000)\n",
    "figure(figsize=(3,3))\n",
    "plot_predictions(xs, ys, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results look reasonable, both within the interval of observed data and in the extrapolated predictions on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the same experiment run with following data set, which has significantly more noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_noisy = [5.092, 4.781, 2.46815, 1.23047, 0.903318, 1.11819, 2.10808, 1.09198, 0.0203789, -2.05068, 2.66031];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(3,3))\n",
    "scatter(xs, ys_noisy, color=\"red\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pred_ys = infer_and_predict(line_model, xs, ys_noisy, new_xs, [:slope, :intercept], 20, 1000)\n",
    "figure(figsize=(3,3))\n",
    "plot_predictions(xs, ys_noisy, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the generated data is less noisy than the observed data in the regime where data was observed, and it looks like the forecasted data is too overconfident. This is a sign that our model is mis-specified. In our case, this is because we have assumed that the noise has value 0.1. However, the actual noise in the data appears to be much larger. We can correct this by making the noise a random choice as well and inferring its value along with the other parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Write a new version of the line model that samples a random choice for the noise from a `gamma(1, 1)` prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@gen function line_model_2(xs::Vector{Float64})\n",
    "    n = length(xs)\n",
    "    slope = @trace(normal(0, 1), :slope)\n",
    "    intercept = @trace(normal(0, 2), :intercept)\n",
    "    \n",
    "    # <your code here>\n",
    "    \n",
    "    for (i, x) in enumerate(xs)\n",
    "        @trace(normal(slope * x + intercept, noise), (:y, i))\n",
    "    end\n",
    "    return nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Line Model with Noise](images/line_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we compare the predictions using inference with the unmodified and modified model on the `ys` data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(6,3))\n",
    "\n",
    "pred_ys = infer_and_predict(line_model, xs, ys, new_xs, [:slope, :intercept], 20, 1000)\n",
    "subplot(1, 2, 1)\n",
    "title(\"Fixed noise level\")\n",
    "plot_predictions(xs, ys, new_xs, pred_ys)\n",
    "\n",
    "pred_ys = infer_and_predict(line_model_2, xs, ys, new_xs, [:slope, :intercept, :noise], 20, 10000)\n",
    "subplot(1, 2, 2)\n",
    "title(\"Inferred noise level\")\n",
    "plot_predictions(xs, ys, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is more uncertainty in the predictions made using the modified model.\n",
    "\n",
    "We also compare the predictions using inference the unmodified and modified model on the `ys_noisy` data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "figure(figsize=(6,3))\n",
    "\n",
    "pred_ys = infer_and_predict(line_model, xs, ys_noisy, new_xs, [:slope, :intercept], 20, 1000)\n",
    "subplot(1, 2, 1)\n",
    "title(\"Fixed noise level\")\n",
    "plot_predictions(xs, ys_noisy, new_xs, pred_ys)\n",
    "\n",
    "pred_ys = infer_and_predict(line_model_2, xs, ys_noisy, new_xs, [:slope, :intercept, :noise], 20, 10000)\n",
    "subplot(1, 2, 2)\n",
    "title(\"Inferred noise level\")\n",
    "plot_predictions(xs, ys_noisy, new_xs, pred_ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that while the unmodified model was very overconfident, the modified model has an appropriate level of uncertainty, while still capturing the general negative trend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generative models in visual object perception - a teaser  <a name=\"shape-inference\"></a>\n",
    "\n",
    "In this section, we will have a look at a more complicated example in the domain of visual perception to illustrate the power of Gen and even basic inference algorithms such as importance resampling.\n",
    "\n",
    "Let's have a look at the generative model illustrated below:\n",
    "\n",
    "<img src=\"./images/shape_inference_pipeline.jpg\" alt=\"Adapted from https://github.com/junyanz/VON\" width=\"800\"/>\n",
    "\n",
    "In this generative model, a \"shape network\" that is based on extended versions of Generative Adversarial Networks (GAN) is trained to generate 3D clouds from a **shape code**. A renderer (\"differentiable projection\") then constructs 2.5D images (mask and depth) given a **view point**.\n",
    "\n",
    "Such a framework is powerful as we can use it in at least two ways:\n",
    "1. We can synthesize a vast number of images varying object category, shape code and view point.\n",
    "2. We can observe images and infer/analyze what the underlying object category, shape code and view point must have been.\n",
    "\n",
    "2 is often referred to as \"perception by analysis by synthesis\", and this idea is used to model human and animal perception, in and beyond vision.\n",
    "\n",
    "While especially 2 may seem difficult at first sight, after this module only we already have sufficient knowledge in modeling in Gen to create such an analysis by synthesis framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the Gen implementation of that generative model:\n",
    "\n",
    "```julia\n",
    "@gen function generative_model()\n",
    "    obj_id = @trace(categorical([0.5, 0.5]), :obj_id) # car or chair\n",
    "    obj_model = select_model(obj_id)\n",
    "    shape_code = @trace(mvnormal(zeros(200), Diagonal(ones(200))), :shape_code)\n",
    "    azimuth = @trace(uniform_discrete(-90, 90), :azimuth)\n",
    "    img_mask, img_depth = obj_model.sample_2d(shape_code, [0, azimuth])\n",
    "    img_mask  = img_mask[1,:,:]\n",
    "    img_depth = img_depth[1,:,:]\n",
    "    blurred = gaussian_blur(img_depth)\n",
    "    observable = @trace(pixel_noise(blurred, 0.1), :observable)\n",
    "\n",
    "    return (img_mask, img_depth, blurred, observable)\n",
    "end\n",
    "``` \n",
    "\n",
    "And the inference procedure:\n",
    "\n",
    "```julia\n",
    "# run sampling importance resampling\n",
    "function generic_importance_resampling(generative_model, observable, n_iterations)\n",
    "    constraints = choicemap((:observable, observable))\n",
    "    (trace, _) = importance_resampling(generative_model, (), constraints, n_iterations)\n",
    "    return trace\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate the observation, i.e. inferring the latents in the generative model given an observation, of the following depth image.\n",
    "\n",
    "<img src=\"./images/shape_inference_observation.png\" alt=\"Observed depth image\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize a few inferred posterior samples (per sample, we ran importance resampling with 20 samples):\n",
    "\n",
    "<img src=\"./images/shape_inference_posterior-20.jpg\" alt=\"Posterior with 20 samples\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the sample size of the inference algorithm, our samples get better at explaining the observed image (100 samples):\n",
    "\n",
    "<img src=\"./images/shape_inference_posterior-100.jpg\" alt=\"Posterior with 100 samples\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally using 500 samples:\n",
    "\n",
    "<img src=\"./images/shape_inference_posterior-500.jpg\" alt=\"Posterior with 500 samples\" width=\"800\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.3",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
